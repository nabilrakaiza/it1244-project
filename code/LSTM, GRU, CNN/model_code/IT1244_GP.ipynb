{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMahY1zfKL6z9i2N8KRiCZA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bSK8UaXxbPbn","executionInfo":{"status":"ok","timestamp":1760943948438,"user_tz":-480,"elapsed":669,"user":{"displayName":"Jaisev","userId":"15130224773038967319"}},"outputId":"a3045c16-7c13-4c58-b104-7543f8a67fcc"},"outputs":[{"output_type":"stream","name":"stdout","text":["trans_1: rows=69680 | groups=104 | train=48176 (72 groups) | val=6720 (10 groups) | test=14784 (22 groups)\n","trans_2: rows=69680 | groups=104 | train=48176 (72 groups) | val=6720 (10 groups) | test=14784 (22 groups)\n","\n","Done. Split indices saved under: ../splits/<transformer>/indices_*.csv\n"]}],"source":["# --- One-cell time-series splitter (70/10/20 by groups, leakage-safe) ---\n","\n","import os, math, numpy as np, pandas as pd\n","\n","# ====== CONFIG (edit as needed) ======\n","TRANSFORMERS = {\n","    \"trans_1\": \"/content/trans_1.csv\",\n","    \"trans_2\": \"/content/trans_2.csv\",\n","}\n","TIMESTAMP_COL = None  # e.g., \"timestamp\" if you have one; else leave as None\n","GROUP_SIZE    = 96 * 7  # contiguous rows per group (e.g., 1 week for hourly data)\n","PROPORTIONS   = (0.70, 0.10, 0.20)  # train, val, test by groups\n","RANDOM_SEED   = 42     # shuffle groups reproducibly\n","OUT_DIR       = \"../splits\"\n","# =====================================\n","\n","def load_df(path, ts_col):\n","    df = pd.read_csv(path)\n","    if ts_col and ts_col in df.columns:\n","        df = df.sort_values(ts_col)\n","    return df.reset_index(drop=True)\n","\n","def make_group_ids(n, g):\n","    n_groups = math.ceil(n / g)\n","    groups = np.repeat(np.arange(n_groups), g)[:n]\n","    return groups, n_groups\n","\n","def split_groups(n_groups, props=(0.7,0.1,0.2), seed=42, randomize=True):\n","    assert abs(sum(props) - 1.0) < 1e-9, \"PROPORTIONS must sum to 1\"\n","    # group ids in chronological order:\n","    ids = np.arange(n_groups)\n","    if randomize:\n","        rng = np.random.RandomState(seed)\n","        rng.shuffle(ids)\n","    n_train = int(math.floor(props[0] * n_groups))\n","    n_val   = int(math.floor(props[1] * n_groups))\n","    n_test  = max(0, n_groups - n_train - n_val)\n","    train_ids = np.sort(ids[:n_train])\n","    val_ids   = np.sort(ids[n_train:n_train+n_val])\n","    test_ids  = np.sort(ids[n_train+n_val:n_train+n_val+n_test])\n","    return train_ids, val_ids, test_ids\n","\n","def indices_for(groups, chosen):\n","    return np.where(np.isin(groups, chosen))[0]\n","\n","def save_indices(base, split_name, idx):\n","    os.makedirs(base, exist_ok=True)\n","    pd.Series(idx, name=\"row_index\").to_csv(os.path.join(base, f\"indices_{split_name}.csv\"), index=False)\n","\n","for tf_name, path in TRANSFORMERS.items():\n","    df = load_df(path, TIMESTAMP_COL)\n","    groups, n_groups = make_group_ids(len(df), GROUP_SIZE)\n","    g_tr, g_va, g_te = split_groups(n_groups, PROPORTIONS, RANDOM_SEED, randomize=True)\n","\n","    idx_tr = indices_for(groups, g_tr)\n","    idx_va = indices_for(groups, g_va)\n","    idx_te = indices_for(groups, g_te)\n","\n","    base = os.path.join(OUT_DIR, tf_name)\n","    save_indices(base, \"train\", idx_tr)\n","    save_indices(base, \"val\",   idx_va)\n","    save_indices(base, \"test\",  idx_te)\n","\n","    # quick sanity report\n","    print(\n","        f\"{tf_name}: rows={len(df)} | groups={n_groups} | \"\n","        f\"train={len(idx_tr)} ({len(g_tr)} groups) | \"\n","        f\"val={len(idx_va)} ({len(g_va)} groups) | \"\n","        f\"test={len(idx_te)} ({len(g_te)} groups)\"\n","    )\n","\n","print(f\"\\nDone. Split indices saved under: {OUT_DIR}/<transformer>/indices_*.csv\")\n"]}]}