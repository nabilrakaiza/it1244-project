{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filename, N, points_prior):\n",
    "    \"\"\"\n",
    "    Preprocesses a time series dataset and sets up time series cross-validation folds.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to CSV file.\n",
    "        N (int): Window size for rolling and lag features.\n",
    "        points_prior (int): Number of points to shift for prediction target.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame)): DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    feature_columns = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL']\n",
    "\n",
    "    # Rolling and lag features\n",
    "    for col in feature_columns:\n",
    "        df[f\"rolling_mean_{col}\"] = df[col].rolling(window=N).mean().shift(points_prior)\n",
    "        df[f\"rolling_std_{col}\"] = df[col].rolling(window=N).std().shift(points_prior)\n",
    "        for i in range(1, N + 1):\n",
    "            df[f'{col}_lag{i + points_prior}'] = df[col].shift(i + points_prior)\n",
    "\n",
    "    # Drop current columns and missing values\n",
    "    df = df.drop(columns=feature_columns)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Time feature extraction\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"day\"] = df[\"date\"].dt.day\n",
    "    df[\"dow\"] = df[\"date\"].dt.day_of_week\n",
    "    df[\"hour\"] = df[\"date\"].dt.hour\n",
    "    df[\"minute\"] = df[\"date\"].dt.minute\n",
    "    df = df.drop(columns=\"date\")\n",
    "\n",
    "    # Memory optimization\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = df[col].astype('float32')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = df[col].astype('int32')\n",
    "\n",
    "    return df\n",
    "\n",
    "def time_series_split(df, target_col, n_splits=5, max_train_size=None, test_ratio = 0.20):\n",
    "    # Target variable (example: predict next HUFL)\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    n_total = len(df)\n",
    "    n_test = int(n_total * test_ratio)\n",
    "    split_idx = n_total - n_test\n",
    "\n",
    "    X_trainval, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_trainval, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    print(f\"Train/Val samples: {len(X_trainval)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "    # ---- Time Series Cross-Validation for train/val ----\n",
    "    tscv = TimeSeriesSplit(n_splits=3, max_train_size=max_train_size)\n",
    "\n",
    "    return X_trainval, y_trainval, X_test, y_test, tscv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================\n",
      "Processing: File=trans_1.csv, PointsPrior=4, Model=CatBoost Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55732, Test samples: 13932\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 12\n",
      "points_prior: 4\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag15', 'HUFL_lag16', 'rolling_mean_MUFL', 'MUFL_lag14', 'MUFL_lag15', 'MUFL_lag16', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag7', 'MULL_lag8', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag5', 'LULL_lag6', 'LULL_lag16', 'year', 'month', 'day']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 600}\n",
      "Best CV RMSE: 12.191928796297713\n",
      "Final Test RMSE: 4.7769\n",
      "Train/Val samples: 55722, Test samples: 13930\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 24\n",
      "points_prior: 4\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag27', 'HUFL_lag28', 'rolling_mean_MUFL', 'rolling_std_MUFL', 'MUFL_lag7', 'MUFL_lag14', 'MUFL_lag28', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag8', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag15', 'LULL_lag17', 'LULL_lag23', 'year', 'month', 'day']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 300}\n",
      "Best CV RMSE: 12.405083785053078\n",
      "Final Test RMSE: 5.1789\n",
      "Train/Val samples: 55703, Test samples: 13925\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 48\n",
      "points_prior: 4\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'rolling_std_HUFL', 'HUFL_lag41', 'rolling_mean_HULL', 'rolling_mean_MUFL', 'rolling_std_MUFL', 'MUFL_lag5', 'MUFL_lag43', 'MUFL_lag45', 'MUFL_lag46', 'rolling_mean_MULL', 'MULL_lag6', 'rolling_mean_LUFL', 'LULL_lag11', 'LULL_lag14', 'LULL_lag16', 'LULL_lag18', 'year', 'month', 'day']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 600}\n",
      "Best CV RMSE: 12.68389798850896\n",
      "Final Test RMSE: 5.5356\n",
      "Model saved to Model_CatBoost Regressor N_12 points_prior_4 dataset_trans_1.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_1.csv, points_prior: 4, Model CatBoost Regressor ***\n",
      "Best N: \t\t12\n",
      "Best Test RMSE: \t4.7769\n",
      "Best Hyperparameters: \t{'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 600}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_1.csv, PointsPrior=96, Model=CatBoost Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55658, Test samples: 13914\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 12\n",
      "points_prior: 96\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag108', 'rolling_mean_MUFL', 'MUFL_lag101', 'MUFL_lag104', 'MUFL_lag105', 'MUFL_lag106', 'MUFL_lag107', 'MUFL_lag108', 'rolling_mean_MULL', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag97', 'LULL_lag98', 'LULL_lag100', 'LULL_lag108', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 6, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 300}\n",
      "Best CV RMSE: 12.853871828015198\n",
      "Final Test RMSE: 5.6524\n",
      "Train/Val samples: 55648, Test samples: 13912\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 24\n",
      "points_prior: 96\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'rolling_mean_MUFL', 'rolling_std_MUFL', 'MUFL_lag99', 'MUFL_lag104', 'MUFL_lag108', 'MUFL_lag109', 'MUFL_lag110', 'MUFL_lag115', 'MUFL_lag120', 'rolling_mean_MULL', 'rolling_mean_LUFL', 'LULL_lag97', 'LULL_lag98', 'LULL_lag120', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 6, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 600}\n",
      "Best CV RMSE: 13.047144978620905\n",
      "Final Test RMSE: 5.4113\n",
      "Train/Val samples: 55629, Test samples: 13907\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 48\n",
      "points_prior: 96\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_std_HUFL', 'rolling_std_MUFL', 'MUFL_lag106', 'MUFL_lag107', 'MUFL_lag109', 'MUFL_lag110', 'MUFL_lag112', 'MUFL_lag136', 'MUFL_lag137', 'MUFL_lag140', 'MUFL_lag142', 'rolling_mean_MULL', 'LULL_lag139', 'LULL_lag142', 'LULL_lag143', 'LULL_lag144', 'year', 'month', 'day', 'dow']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 6, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 600}\n",
      "Best CV RMSE: 13.450738840655973\n",
      "Final Test RMSE: 5.8090\n",
      "Model saved to Model_CatBoost Regressor N_24 points_prior_96 dataset_trans_1.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_1.csv, points_prior: 96, Model CatBoost Regressor ***\n",
      "Best N: \t\t24\n",
      "Best Test RMSE: \t5.4113\n",
      "Best Hyperparameters: \t{'model__depth': 6, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 600}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_1.csv, PointsPrior=672, Model=CatBoost Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55197, Test samples: 13799\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 12\n",
      "points_prior: 672\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HULL', 'rolling_mean_MUFL', 'MUFL_lag681', 'MUFL_lag682', 'MUFL_lag683', 'MUFL_lag684', 'rolling_mean_MULL', 'MULL_lag673', 'rolling_mean_LUFL', 'LUFL_lag684', 'rolling_mean_LULL', 'LULL_lag673', 'LULL_lag675', 'LULL_lag683', 'LULL_lag684', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 6, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 300}\n",
      "Best CV RMSE: 13.714531860112292\n",
      "Final Test RMSE: 5.1601\n",
      "Train/Val samples: 55188, Test samples: 13796\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 24\n",
      "points_prior: 672\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HULL', 'rolling_mean_MUFL', 'MUFL_lag678', 'MUFL_lag679', 'MUFL_lag680', 'MUFL_lag681', 'MUFL_lag683', 'MUFL_lag685', 'MUFL_lag686', 'MUFL_lag688', 'rolling_mean_MULL', 'MULL_lag673', 'rolling_mean_LULL', 'LULL_lag673', 'LULL_lag696', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 6, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 300}\n",
      "Best CV RMSE: 13.961405758606249\n",
      "Final Test RMSE: 5.2732\n",
      "Train/Val samples: 55168, Test samples: 13792\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 48\n",
      "points_prior: 672\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HULL', 'MUFL_lag679', 'MUFL_lag683', 'MUFL_lag684', 'MUFL_lag685', 'MUFL_lag687', 'MUFL_lag688', 'rolling_mean_MULL', 'MULL_lag673', 'MULL_lag674', 'rolling_mean_LUFL', 'LULL_lag673', 'LULL_lag715', 'LULL_lag716', 'LULL_lag719', 'LULL_lag720', 'year', 'month', 'day', 'dow']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 600}\n",
      "Best CV RMSE: 13.989107126243677\n",
      "Final Test RMSE: 4.6232\n",
      "Model saved to Model_CatBoost Regressor N_48 points_prior_672 dataset_trans_1.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_1.csv, points_prior: 672, Model CatBoost Regressor ***\n",
      "Best N: \t\t48\n",
      "Best Test RMSE: \t4.6232\n",
      "Best Hyperparameters: \t{'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 600}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_2.csv, PointsPrior=4, Model=CatBoost Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55732, Test samples: 13932\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 12\n",
      "points_prior: 4\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag5', 'HUFL_lag6', 'HUFL_lag8', 'HUFL_lag9', 'rolling_mean_MUFL', 'MUFL_lag5', 'MUFL_lag6', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag14', 'MULL_lag16', 'rolling_mean_LUFL', 'LUFL_lag5', 'LUFL_lag6', 'rolling_mean_LULL', 'year', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 300}\n",
      "Best CV RMSE: 12.530104892245575\n",
      "Final Test RMSE: 5.4333\n",
      "Train/Val samples: 55722, Test samples: 13930\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 24\n",
      "points_prior: 4\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag12', 'rolling_std_MUFL', 'MUFL_lag5', 'MUFL_lag6', 'MUFL_lag7', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag7', 'MULL_lag8', 'MULL_lag9', 'MULL_lag10', 'MULL_lag11', 'MULL_lag13', 'LUFL_lag5', 'rolling_mean_LULL', 'year', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 300}\n",
      "Best CV RMSE: 12.449735137745657\n",
      "Final Test RMSE: 5.4323\n",
      "Train/Val samples: 55703, Test samples: 13925\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 48\n",
      "points_prior: 4\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_std_MUFL', 'MUFL_lag5', 'MUFL_lag6', 'MUFL_lag7', 'MUFL_lag42', 'MUFL_lag44', 'MUFL_lag45', 'MUFL_lag47', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag7', 'MULL_lag8', 'MULL_lag9', 'MULL_lag10', 'MULL_lag11', 'year', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 300}\n",
      "Best CV RMSE: 13.274086272562016\n",
      "Final Test RMSE: 5.5422\n",
      "Model saved to Model_CatBoost Regressor N_24 points_prior_4 dataset_trans_2.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_2.csv, points_prior: 4, Model CatBoost Regressor ***\n",
      "Best N: \t\t24\n",
      "Best Test RMSE: \t5.4323\n",
      "Best Hyperparameters: \t{'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 300}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_2.csv, PointsPrior=96, Model=CatBoost Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55658, Test samples: 13914\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 12\n",
      "points_prior: 96\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag97', 'rolling_mean_HULL', 'HULL_lag97', 'rolling_mean_MUFL', 'MUFL_lag97', 'MUFL_lag98', 'MUFL_lag99', 'rolling_mean_MULL', 'MULL_lag99', 'MULL_lag107', 'MULL_lag108', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag97', 'LULL_lag107', 'LULL_lag108', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 300}\n",
      "Best CV RMSE: 12.562942108780467\n",
      "Final Test RMSE: 5.5922\n",
      "Train/Val samples: 55648, Test samples: 13912\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 24\n",
      "points_prior: 96\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_MUFL', 'MUFL_lag97', 'MUFL_lag98', 'MUFL_lag99', 'MUFL_lag100', 'rolling_mean_MULL', 'MULL_lag97', 'MULL_lag98', 'MULL_lag100', 'MULL_lag120', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag110', 'LULL_lag112', 'LULL_lag120', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 300}\n",
      "Best CV RMSE: 12.968467024434329\n",
      "Final Test RMSE: 5.5296\n",
      "Train/Val samples: 55629, Test samples: 13907\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 48\n",
      "points_prior: 96\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_std_MUFL', 'MUFL_lag97', 'MUFL_lag98', 'MUFL_lag139', 'MUFL_lag140', 'MUFL_lag141', 'MUFL_lag142', 'MUFL_lag144', 'rolling_mean_MULL', 'MULL_lag97', 'MULL_lag98', 'MULL_lag100', 'MULL_lag101', 'MULL_lag103', 'MULL_lag105', 'MULL_lag106', 'MULL_lag107', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 300}\n",
      "Best CV RMSE: 12.72323601407786\n",
      "Final Test RMSE: 5.7124\n",
      "Model saved to Model_CatBoost Regressor N_24 points_prior_96 dataset_trans_2.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_2.csv, points_prior: 96, Model CatBoost Regressor ***\n",
      "Best N: \t\t24\n",
      "Best Test RMSE: \t5.5296\n",
      "Best Hyperparameters: \t{'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 300}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_2.csv, PointsPrior=672, Model=CatBoost Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55197, Test samples: 13799\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 12\n",
      "points_prior: 672\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag673', 'rolling_mean_HULL', 'HULL_lag673', 'HULL_lag683', 'HULL_lag684', 'MUFL_lag673', 'rolling_mean_MULL', 'MULL_lag673', 'rolling_mean_LULL', 'LULL_lag673', 'LULL_lag674', 'LULL_lag679', 'LULL_lag683', 'LULL_lag684', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 6, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.03, 'model__n_estimators': 600}\n",
      "Best CV RMSE: 13.944463900618192\n",
      "Final Test RMSE: 5.9043\n",
      "Train/Val samples: 55188, Test samples: 13796\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 24\n",
      "points_prior: 672\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag673', 'HUFL_lag674', 'rolling_mean_HULL', 'HULL_lag673', 'HULL_lag674', 'HULL_lag692', 'HULL_lag695', 'HULL_lag696', 'rolling_mean_MULL', 'MULL_lag673', 'LUFL_lag687', 'LULL_lag673', 'LULL_lag688', 'LULL_lag694', 'LULL_lag695', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 4, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 600}\n",
      "Best CV RMSE: 14.063062468953154\n",
      "Final Test RMSE: 6.1916\n",
      "Train/Val samples: 55168, Test samples: 13792\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 48\n",
      "points_prior: 672\n",
      "model: CatBoost Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag673', 'rolling_mean_HULL', 'HULL_lag673', 'HULL_lag674', 'HULL_lag675', 'HULL_lag676', 'HULL_lag677', 'HULL_lag679', 'HULL_lag680', 'HULL_lag682', 'HULL_lag683', 'MUFL_lag709', 'MUFL_lag710', 'MUFL_lag716', 'rolling_mean_MULL', 'MULL_lag673', 'MULL_lag674', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__depth': 6, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 600}\n",
      "Best CV RMSE: 13.439604517602167\n",
      "Final Test RMSE: 5.8292\n",
      "Model saved to Model_CatBoost Regressor N_48 points_prior_672 dataset_trans_2.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_2.csv, points_prior: 672, Model CatBoost Regressor ***\n",
      "Best N: \t\t48\n",
      "Best Test RMSE: \t5.8292\n",
      "Best Hyperparameters: \t{'model__depth': 6, 'model__l2_leaf_reg': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 600}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "           FINAL SUMMARY           \n",
      "===================================\n",
      "Config (File, Prior, Model): ('trans_1.csv', 4, 'CatBoost Regressor')\n",
      "  -> Best N: 12 (RMSE: 4.7769)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_1.csv', 96, 'CatBoost Regressor')\n",
      "  -> Best N: 24 (RMSE: 5.4113)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_1.csv', 672, 'CatBoost Regressor')\n",
      "  -> Best N: 48 (RMSE: 4.6232)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_2.csv', 4, 'CatBoost Regressor')\n",
      "  -> Best N: 24 (RMSE: 5.4323)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_2.csv', 96, 'CatBoost Regressor')\n",
      "  -> Best N: 24 (RMSE: 5.5296)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_2.csv', 672, 'CatBoost Regressor')\n",
      "  -> Best N: 48 (RMSE: 5.8292)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "models = [\n",
    "        [\"CatBoost Regressor\",\n",
    "     Pipeline([\n",
    "         ('scaler', StandardScaler()),\n",
    "         ('model', CatBoostRegressor(\n",
    "             random_state=42,\n",
    "             verbose=0,        # silence CatBoost logs\n",
    "             loss_function='RMSE'\n",
    "         ))\n",
    "     ]),\n",
    "     {\n",
    "         \"model__depth\": [4, 6],\n",
    "         \"model__learning_rate\": [0.03, 0.1],\n",
    "         \"model__n_estimators\": [300, 600],\n",
    "         \"model__l2_leaf_reg\": [3],\n",
    "     }]\n",
    "]\n",
    "\n",
    "N_list = [12, 24, 48] # can add 96 if you want (and have enough time)\n",
    "points_priors = [4, 96, 672]\n",
    "filenames = [\"trans_1.csv\", \"trans_2.csv\"]\n",
    "\n",
    "# Store the overall best N for each configuration\n",
    "overall_best_results = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    for points_prior in points_priors:\n",
    "        for (model_name, model, model_params) in models:\n",
    "\n",
    "            print(f\"\\n========================================================\")\n",
    "            print(f\"Processing: File={filename}, PointsPrior={points_prior}, Model={model_name}\")\n",
    "            print(f\"========================================================\\n\")\n",
    "\n",
    "            results_per_n = {}\n",
    "            for N in N_list:\n",
    "                df = preprocess_data(filename, N = N, points_prior= points_prior)\n",
    "                X_trainval, y_trainval, X_test, y_test, tscv = time_series_split(df, \"OT\")\n",
    "\n",
    "                print(\"Training Configurations\")\n",
    "                print(f\"file: {filename}\")\n",
    "                print(f\"N: {N}\")\n",
    "                print(f\"points_prior: {points_prior}\")\n",
    "                print(f\"model: {model_name}\")\n",
    "\n",
    "                # ---- FEATURE SELECTION using RFE ----\n",
    "                base_model = XGBRegressor(\n",
    "                    objective='reg:squarederror',\n",
    "                    random_state=42,\n",
    "                    n_estimators=200,\n",
    "                    learning_rate=0.05,\n",
    "                    max_depth=5,\n",
    "                    subsample=0.8\n",
    "                )\n",
    "\n",
    "                # Select top 20 features\n",
    "                print(\"Selecting top 20 features...\")\n",
    "                rfe = RFE(estimator=base_model, n_features_to_select=20, step=0.1)\n",
    "                rfe.fit(X_trainval, y_trainval)\n",
    "\n",
    "                # Mask of selected features\n",
    "                selected_features = list(X_trainval.columns[rfe.support_])\n",
    "                print(\"Top 20 selected features:\\n\", list(selected_features))\n",
    "\n",
    "                # Filter columns\n",
    "                X_trainval_sel = X_trainval[selected_features]\n",
    "                X_test_sel = X_test[selected_features]\n",
    "\n",
    "                # GridSearchCV with TimeSeriesSplit\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_grid=model_params,\n",
    "                    cv=tscv,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "\n",
    "                # Fit using only train/val (no test!)\n",
    "                print(\"Training data using train and val...\")\n",
    "                grid_search.fit(X_trainval_sel, y_trainval)\n",
    "\n",
    "                # Show CV results\n",
    "                print(\"Best parameters:\", grid_search.best_params_)\n",
    "                print(\"Best CV RMSE:\", np.sqrt(-grid_search.best_score_))\n",
    "\n",
    "                # ---- Final evaluation on the hold-out test set ----\n",
    "                best_model = grid_search.best_estimator_\n",
    "                y_pred = best_model.predict(X_test_sel)\n",
    "                test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "                print(f\"Final Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "                results_per_n[N] = {\n",
    "                    \"rmse\": test_rmse,\n",
    "                    \"best_params\": grid_search.best_params_,\n",
    "                    \"selected_features\": selected_features,\n",
    "                    \"best_model\" : best_model\n",
    "                }\n",
    "\n",
    "            best_n_value = min(results_per_n.keys(), key=lambda n: results_per_n[n][\"rmse\"])\n",
    "            best_result = results_per_n[best_n_value]\n",
    "            best_model = best_result[\"best_model\"]\n",
    "\n",
    "            # saving best model\n",
    "            model_filename = f\"Model_{model_name} N_{best_n_value} points_prior_{points_prior} dataset_{filename}.pkl\"\n",
    "            with open(model_filename, 'wb') as file:\n",
    "                pickle.dump(best_model, file)\n",
    "\n",
    "            print(f\"Model saved to {model_filename}\")\n",
    "            \n",
    "            print(f\"\\n--------------------------------------------------------\")\n",
    "            print(f\"*** BEST N for {filename}, points_prior: {points_prior}, Model {model_name} ***\")\n",
    "            print(f\"Best N: \\t\\t{best_n_value}\")\n",
    "            print(f\"Best Test RMSE: \\t{best_result['rmse']:.4f}\")\n",
    "            print(f\"Best Hyperparameters: \\t{best_result['best_params']}\")\n",
    "            print(f\"--------------------------------------------------------\\n\")\n",
    "            \n",
    "            # Store this in the overall results for a final summary\n",
    "            config_key = (filename, points_prior, model_name)\n",
    "            overall_best_results[config_key] = {\n",
    "                \"best_n\": best_n_value,\n",
    "                \"rmse\": best_result['rmse'],\n",
    "                \"best_params\": best_result['best_params'],\n",
    "                \"selected_features\": best_result['selected_features']\n",
    "            }\n",
    "\n",
    "# 4. Print a final summary of all configurations\n",
    "print(\"\\n\\n===================================\")\n",
    "print(\"           FINAL SUMMARY           \")\n",
    "print(\"===================================\")\n",
    "for config, result in overall_best_results.items():\n",
    "    print(f\"Config (File, Prior, Model): {config}\")\n",
    "    print(f\"  -> Best N: {result['best_n']} (RMSE: {result['rmse']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================\n",
      "Processing: File=trans_1.csv, PointsPrior=4, Model=Polynomial Regression (Degree 2)\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55732, Test samples: 13932\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 12\n",
      "points_prior: 4\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag15', 'HUFL_lag16', 'rolling_mean_MUFL', 'MUFL_lag14', 'MUFL_lag15', 'MUFL_lag16', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag7', 'MULL_lag8', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag5', 'LULL_lag6', 'LULL_lag16', 'year', 'month', 'day']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 28.855211123326196\n",
      "Final Test RMSE: 9.8326\n",
      "Train/Val samples: 55722, Test samples: 13930\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 24\n",
      "points_prior: 4\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag27', 'HUFL_lag28', 'rolling_mean_MUFL', 'rolling_std_MUFL', 'MUFL_lag7', 'MUFL_lag14', 'MUFL_lag28', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag8', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag15', 'LULL_lag17', 'LULL_lag23', 'year', 'month', 'day']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 30.19095271943049\n",
      "Final Test RMSE: 9.9754\n",
      "Train/Val samples: 55703, Test samples: 13925\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 48\n",
      "points_prior: 4\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'rolling_std_HUFL', 'HUFL_lag41', 'rolling_mean_HULL', 'rolling_mean_MUFL', 'rolling_std_MUFL', 'MUFL_lag5', 'MUFL_lag43', 'MUFL_lag45', 'MUFL_lag46', 'rolling_mean_MULL', 'MULL_lag6', 'rolling_mean_LUFL', 'LULL_lag11', 'LULL_lag14', 'LULL_lag16', 'LULL_lag18', 'year', 'month', 'day']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 32.61590358201389\n",
      "Final Test RMSE: 9.7473\n",
      "Model saved to Model_Polynomial Regression (Degree 2) N_48 points_prior_4 dataset_trans_1.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_1.csv, points_prior: 4, Model Polynomial Regression (Degree 2) ***\n",
      "Best N: \t\t48\n",
      "Best Test RMSE: \t9.7473\n",
      "Best Hyperparameters: \t{'model__alpha': 10.0}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_1.csv, PointsPrior=96, Model=Polynomial Regression (Degree 2)\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55658, Test samples: 13914\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 12\n",
      "points_prior: 96\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag108', 'rolling_mean_MUFL', 'MUFL_lag101', 'MUFL_lag104', 'MUFL_lag105', 'MUFL_lag106', 'MUFL_lag107', 'MUFL_lag108', 'rolling_mean_MULL', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag97', 'LULL_lag98', 'LULL_lag100', 'LULL_lag108', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 34.096981047618584\n",
      "Final Test RMSE: 9.5042\n",
      "Train/Val samples: 55648, Test samples: 13912\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 24\n",
      "points_prior: 96\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'rolling_mean_MUFL', 'rolling_std_MUFL', 'MUFL_lag99', 'MUFL_lag104', 'MUFL_lag108', 'MUFL_lag109', 'MUFL_lag110', 'MUFL_lag115', 'MUFL_lag120', 'rolling_mean_MULL', 'rolling_mean_LUFL', 'LULL_lag97', 'LULL_lag98', 'LULL_lag120', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 34.46151482032547\n",
      "Final Test RMSE: 9.7260\n",
      "Train/Val samples: 55629, Test samples: 13907\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 48\n",
      "points_prior: 96\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_std_HUFL', 'rolling_std_MUFL', 'MUFL_lag106', 'MUFL_lag107', 'MUFL_lag109', 'MUFL_lag110', 'MUFL_lag112', 'MUFL_lag136', 'MUFL_lag137', 'MUFL_lag140', 'MUFL_lag142', 'rolling_mean_MULL', 'LULL_lag139', 'LULL_lag142', 'LULL_lag143', 'LULL_lag144', 'year', 'month', 'day', 'dow']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 35.23348743647059\n",
      "Final Test RMSE: 10.2497\n",
      "Model saved to Model_Polynomial Regression (Degree 2) N_12 points_prior_96 dataset_trans_1.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_1.csv, points_prior: 96, Model Polynomial Regression (Degree 2) ***\n",
      "Best N: \t\t12\n",
      "Best Test RMSE: \t9.5042\n",
      "Best Hyperparameters: \t{'model__alpha': 10.0}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_1.csv, PointsPrior=672, Model=Polynomial Regression (Degree 2)\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55197, Test samples: 13799\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 12\n",
      "points_prior: 672\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HULL', 'rolling_mean_MUFL', 'MUFL_lag681', 'MUFL_lag682', 'MUFL_lag683', 'MUFL_lag684', 'rolling_mean_MULL', 'MULL_lag673', 'rolling_mean_LUFL', 'LUFL_lag684', 'rolling_mean_LULL', 'LULL_lag673', 'LULL_lag675', 'LULL_lag683', 'LULL_lag684', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 38.44400815659945\n",
      "Final Test RMSE: 10.0231\n",
      "Train/Val samples: 55188, Test samples: 13796\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 24\n",
      "points_prior: 672\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HULL', 'rolling_mean_MUFL', 'MUFL_lag678', 'MUFL_lag679', 'MUFL_lag680', 'MUFL_lag681', 'MUFL_lag683', 'MUFL_lag685', 'MUFL_lag686', 'MUFL_lag688', 'rolling_mean_MULL', 'MULL_lag673', 'rolling_mean_LULL', 'LULL_lag673', 'LULL_lag696', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 38.12651605935128\n",
      "Final Test RMSE: 10.5037\n",
      "Train/Val samples: 55168, Test samples: 13792\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 48\n",
      "points_prior: 672\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HULL', 'MUFL_lag679', 'MUFL_lag683', 'MUFL_lag684', 'MUFL_lag685', 'MUFL_lag687', 'MUFL_lag688', 'rolling_mean_MULL', 'MULL_lag673', 'MULL_lag674', 'rolling_mean_LUFL', 'LULL_lag673', 'LULL_lag715', 'LULL_lag716', 'LULL_lag719', 'LULL_lag720', 'year', 'month', 'day', 'dow']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 40.85034115219749\n",
      "Final Test RMSE: 8.8322\n",
      "Model saved to Model_Polynomial Regression (Degree 2) N_48 points_prior_672 dataset_trans_1.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_1.csv, points_prior: 672, Model Polynomial Regression (Degree 2) ***\n",
      "Best N: \t\t48\n",
      "Best Test RMSE: \t8.8322\n",
      "Best Hyperparameters: \t{'model__alpha': 10.0}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_2.csv, PointsPrior=4, Model=Polynomial Regression (Degree 2)\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55732, Test samples: 13932\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 12\n",
      "points_prior: 4\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag5', 'HUFL_lag6', 'HUFL_lag8', 'HUFL_lag9', 'rolling_mean_MUFL', 'MUFL_lag5', 'MUFL_lag6', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag14', 'MULL_lag16', 'rolling_mean_LUFL', 'LUFL_lag5', 'LUFL_lag6', 'rolling_mean_LULL', 'year', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 130.41229206076486\n",
      "Final Test RMSE: 7.3243\n",
      "Train/Val samples: 55722, Test samples: 13930\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 24\n",
      "points_prior: 4\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag12', 'rolling_std_MUFL', 'MUFL_lag5', 'MUFL_lag6', 'MUFL_lag7', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag7', 'MULL_lag8', 'MULL_lag9', 'MULL_lag10', 'MULL_lag11', 'MULL_lag13', 'LUFL_lag5', 'rolling_mean_LULL', 'year', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 44.646828114867496\n",
      "Final Test RMSE: 11.1676\n",
      "Train/Val samples: 55703, Test samples: 13925\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 48\n",
      "points_prior: 4\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_std_MUFL', 'MUFL_lag5', 'MUFL_lag6', 'MUFL_lag7', 'MUFL_lag42', 'MUFL_lag44', 'MUFL_lag45', 'MUFL_lag47', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag7', 'MULL_lag8', 'MULL_lag9', 'MULL_lag10', 'MULL_lag11', 'year', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 23.343806840928544\n",
      "Final Test RMSE: 20.4504\n",
      "Model saved to Model_Polynomial Regression (Degree 2) N_12 points_prior_4 dataset_trans_2.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_2.csv, points_prior: 4, Model Polynomial Regression (Degree 2) ***\n",
      "Best N: \t\t12\n",
      "Best Test RMSE: \t7.3243\n",
      "Best Hyperparameters: \t{'model__alpha': 10.0}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_2.csv, PointsPrior=96, Model=Polynomial Regression (Degree 2)\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55658, Test samples: 13914\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 12\n",
      "points_prior: 96\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag97', 'rolling_mean_HULL', 'HULL_lag97', 'rolling_mean_MUFL', 'MUFL_lag97', 'MUFL_lag98', 'MUFL_lag99', 'rolling_mean_MULL', 'MULL_lag99', 'MULL_lag107', 'MULL_lag108', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag97', 'LULL_lag107', 'LULL_lag108', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 0.1}\n",
      "Best CV RMSE: 75.00484557525634\n",
      "Final Test RMSE: 6.2414\n",
      "Train/Val samples: 55648, Test samples: 13912\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 24\n",
      "points_prior: 96\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_MUFL', 'MUFL_lag97', 'MUFL_lag98', 'MUFL_lag99', 'MUFL_lag100', 'rolling_mean_MULL', 'MULL_lag97', 'MULL_lag98', 'MULL_lag100', 'MULL_lag120', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag110', 'LULL_lag112', 'LULL_lag120', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 117.14990906482244\n",
      "Final Test RMSE: 14.7283\n",
      "Train/Val samples: 55629, Test samples: 13907\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 48\n",
      "points_prior: 96\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_std_MUFL', 'MUFL_lag97', 'MUFL_lag98', 'MUFL_lag139', 'MUFL_lag140', 'MUFL_lag141', 'MUFL_lag142', 'MUFL_lag144', 'rolling_mean_MULL', 'MULL_lag97', 'MULL_lag98', 'MULL_lag100', 'MULL_lag101', 'MULL_lag103', 'MULL_lag105', 'MULL_lag106', 'MULL_lag107', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 16.317630522675902\n",
      "Final Test RMSE: 6.5044\n",
      "Model saved to Model_Polynomial Regression (Degree 2) N_12 points_prior_96 dataset_trans_2.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_2.csv, points_prior: 96, Model Polynomial Regression (Degree 2) ***\n",
      "Best N: \t\t12\n",
      "Best Test RMSE: \t6.2414\n",
      "Best Hyperparameters: \t{'model__alpha': 0.1}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_2.csv, PointsPrior=672, Model=Polynomial Regression (Degree 2)\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55197, Test samples: 13799\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 12\n",
      "points_prior: 672\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag673', 'rolling_mean_HULL', 'HULL_lag673', 'HULL_lag683', 'HULL_lag684', 'MUFL_lag673', 'rolling_mean_MULL', 'MULL_lag673', 'rolling_mean_LULL', 'LULL_lag673', 'LULL_lag674', 'LULL_lag679', 'LULL_lag683', 'LULL_lag684', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 0.1}\n",
      "Best CV RMSE: 156.8991635923011\n",
      "Final Test RMSE: 21.8189\n",
      "Train/Val samples: 55188, Test samples: 13796\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 24\n",
      "points_prior: 672\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag673', 'HUFL_lag674', 'rolling_mean_HULL', 'HULL_lag673', 'HULL_lag674', 'HULL_lag692', 'HULL_lag695', 'HULL_lag696', 'rolling_mean_MULL', 'MULL_lag673', 'LUFL_lag687', 'LULL_lag673', 'LULL_lag688', 'LULL_lag694', 'LULL_lag695', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 1.0}\n",
      "Best CV RMSE: 35.15719535211539\n",
      "Final Test RMSE: 16.5104\n",
      "Train/Val samples: 55168, Test samples: 13792\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 48\n",
      "points_prior: 672\n",
      "model: Polynomial Regression (Degree 2)\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag673', 'rolling_mean_HULL', 'HULL_lag673', 'HULL_lag674', 'HULL_lag675', 'HULL_lag676', 'HULL_lag677', 'HULL_lag679', 'HULL_lag680', 'HULL_lag682', 'HULL_lag683', 'MUFL_lag709', 'MUFL_lag710', 'MUFL_lag716', 'rolling_mean_MULL', 'MULL_lag673', 'MULL_lag674', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters: {'model__alpha': 10.0}\n",
      "Best CV RMSE: 19.068134355844503\n",
      "Final Test RMSE: 6.6715\n",
      "Model saved to Model_Polynomial Regression (Degree 2) N_48 points_prior_672 dataset_trans_2.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_2.csv, points_prior: 672, Model Polynomial Regression (Degree 2) ***\n",
      "Best N: \t\t48\n",
      "Best Test RMSE: \t6.6715\n",
      "Best Hyperparameters: \t{'model__alpha': 10.0}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "           FINAL SUMMARY           \n",
      "===================================\n",
      "Config (File, Prior, Model): ('trans_1.csv', 4, 'Polynomial Regression (Degree 2)')\n",
      "  -> Best N: 48 (RMSE: 9.7473)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_1.csv', 96, 'Polynomial Regression (Degree 2)')\n",
      "  -> Best N: 12 (RMSE: 9.5042)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_1.csv', 672, 'Polynomial Regression (Degree 2)')\n",
      "  -> Best N: 48 (RMSE: 8.8322)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_2.csv', 4, 'Polynomial Regression (Degree 2)')\n",
      "  -> Best N: 12 (RMSE: 7.3243)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_2.csv', 96, 'Polynomial Regression (Degree 2)')\n",
      "  -> Best N: 12 (RMSE: 6.2414)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_2.csv', 672, 'Polynomial Regression (Degree 2)')\n",
      "  -> Best N: 48 (RMSE: 6.6715)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "models = [\n",
    "    [\n",
    "        \"Polynomial Regression (Degree 2)\",\n",
    "        Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "            ('model', Ridge())\n",
    "        ]),\n",
    "        {\n",
    "            \"model__alpha\": [0.1, 1.0, 10.0]  # regularization strength\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "N_list = [12, 24, 48] # can add 96 if you want (and have enough time)\n",
    "points_priors = [4, 96, 672]\n",
    "filenames = [\"trans_1.csv\", \"trans_2.csv\"]\n",
    "\n",
    "# Store the overall best N for each configuration\n",
    "overall_best_results = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    for points_prior in points_priors:\n",
    "        for (model_name, model, model_params) in models:\n",
    "\n",
    "            print(f\"\\n========================================================\")\n",
    "            print(f\"Processing: File={filename}, PointsPrior={points_prior}, Model={model_name}\")\n",
    "            print(f\"========================================================\\n\")\n",
    "\n",
    "            results_per_n = {}\n",
    "            for N in N_list:\n",
    "                df = preprocess_data(filename, N = N, points_prior= points_prior)\n",
    "                X_trainval, y_trainval, X_test, y_test, tscv = time_series_split(df, \"OT\")\n",
    "\n",
    "                print(\"Training Configurations\")\n",
    "                print(f\"file: {filename}\")\n",
    "                print(f\"N: {N}\")\n",
    "                print(f\"points_prior: {points_prior}\")\n",
    "                print(f\"model: {model_name}\")\n",
    "\n",
    "                # ---- FEATURE SELECTION using RFE ----\n",
    "                base_model = XGBRegressor(\n",
    "                    objective='reg:squarederror',\n",
    "                    random_state=42,\n",
    "                    n_estimators=200,\n",
    "                    learning_rate=0.05,\n",
    "                    max_depth=5,\n",
    "                    subsample=0.8\n",
    "                )\n",
    "\n",
    "                # Select top 20 features\n",
    "                print(\"Selecting top 20 features...\")\n",
    "                rfe = RFE(estimator=base_model, n_features_to_select=20, step=0.1)\n",
    "                rfe.fit(X_trainval, y_trainval)\n",
    "\n",
    "                # Mask of selected features\n",
    "                selected_features = list(X_trainval.columns[rfe.support_])\n",
    "                print(\"Top 20 selected features:\\n\", list(selected_features))\n",
    "\n",
    "                # Filter columns\n",
    "                X_trainval_sel = X_trainval[selected_features]\n",
    "                X_test_sel = X_test[selected_features]\n",
    "\n",
    "                # GridSearchCV with TimeSeriesSplit\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_grid=model_params,\n",
    "                    cv=tscv,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "\n",
    "                # Fit using only train/val (no test!)\n",
    "                print(\"Training data using train and val...\")\n",
    "                grid_search.fit(X_trainval_sel, y_trainval)\n",
    "\n",
    "                # Show CV results\n",
    "                print(\"Best parameters:\", grid_search.best_params_)\n",
    "                print(\"Best CV RMSE:\", np.sqrt(-grid_search.best_score_))\n",
    "\n",
    "                # ---- Final evaluation on the hold-out test set ----\n",
    "                best_model = grid_search.best_estimator_\n",
    "                y_pred = best_model.predict(X_test_sel)\n",
    "                test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "                print(f\"Final Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "                results_per_n[N] = {\n",
    "                    \"rmse\": test_rmse,\n",
    "                    \"best_params\": grid_search.best_params_,\n",
    "                    \"selected_features\": selected_features,\n",
    "                    \"best_model\" : best_model\n",
    "                }\n",
    "\n",
    "            best_n_value = min(results_per_n.keys(), key=lambda n: results_per_n[n][\"rmse\"])\n",
    "            best_result = results_per_n[best_n_value]\n",
    "            best_model = best_result[\"best_model\"]\n",
    "\n",
    "            # saving best model\n",
    "            model_filename = f\"Model_{model_name} N_{best_n_value} points_prior_{points_prior} dataset_{filename}.pkl\"\n",
    "            with open(model_filename, 'wb') as file:\n",
    "                pickle.dump(best_model, file)\n",
    "\n",
    "            print(f\"Model saved to {model_filename}\")\n",
    "            \n",
    "            print(f\"\\n--------------------------------------------------------\")\n",
    "            print(f\"*** BEST N for {filename}, points_prior: {points_prior}, Model {model_name} ***\")\n",
    "            print(f\"Best N: \\t\\t{best_n_value}\")\n",
    "            print(f\"Best Test RMSE: \\t{best_result['rmse']:.4f}\")\n",
    "            print(f\"Best Hyperparameters: \\t{best_result['best_params']}\")\n",
    "            print(f\"--------------------------------------------------------\\n\")\n",
    "            \n",
    "            # Store this in the overall results for a final summary\n",
    "            config_key = (filename, points_prior, model_name)\n",
    "            overall_best_results[config_key] = {\n",
    "                \"best_n\": best_n_value,\n",
    "                \"rmse\": best_result['rmse'],\n",
    "                \"best_params\": best_result['best_params'],\n",
    "                \"selected_features\": best_result['selected_features']\n",
    "            }\n",
    "\n",
    "# 4. Print a final summary of all configurations\n",
    "print(\"\\n\\n===================================\")\n",
    "print(\"           FINAL SUMMARY           \")\n",
    "print(\"===================================\")\n",
    "for config, result in overall_best_results.items():\n",
    "    print(f\"Config (File, Prior, Model): {config}\")\n",
    "    print(f\"  -> Best N: {result['best_n']} (RMSE: {result['rmse']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================\n",
      "Processing: File=trans_1.csv, PointsPrior=4, Model=LightGBM Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55732, Test samples: 13932\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 12\n",
      "points_prior: 4\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag15', 'HUFL_lag16', 'rolling_mean_MUFL', 'MUFL_lag14', 'MUFL_lag15', 'MUFL_lag16', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag7', 'MULL_lag8', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag5', 'LULL_lag6', 'LULL_lag16', 'year', 'month', 'day']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 600, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 12.854742511293642\n",
      "Final Test RMSE: 6.3048\n",
      "Train/Val samples: 55722, Test samples: 13930\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 24\n",
      "points_prior: 4\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag27', 'HUFL_lag28', 'rolling_mean_MUFL', 'rolling_std_MUFL', 'MUFL_lag7', 'MUFL_lag14', 'MUFL_lag28', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag8', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag15', 'LULL_lag17', 'LULL_lag23', 'year', 'month', 'day']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 600, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 13.006267942031041\n",
      "Final Test RMSE: 6.6386\n",
      "Train/Val samples: 55703, Test samples: 13925\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 48\n",
      "points_prior: 4\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'rolling_std_HUFL', 'HUFL_lag41', 'rolling_mean_HULL', 'rolling_mean_MUFL', 'rolling_std_MUFL', 'MUFL_lag5', 'MUFL_lag43', 'MUFL_lag45', 'MUFL_lag46', 'rolling_mean_MULL', 'MULL_lag6', 'rolling_mean_LUFL', 'LULL_lag11', 'LULL_lag14', 'LULL_lag16', 'LULL_lag18', 'year', 'month', 'day']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 600, 'model__num_leaves': 63, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 13.284078591814755\n",
      "Final Test RMSE: 7.1501\n",
      "Model saved to Model_LightGBM Regressor N_12 points_prior_4 dataset_trans_1.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_1.csv, points_prior: 4, Model LightGBM Regressor ***\n",
      "Best N: \t\t12\n",
      "Best Test RMSE: \t6.3048\n",
      "Best Hyperparameters: \t{'model__learning_rate': 0.1, 'model__n_estimators': 600, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_1.csv, PointsPrior=96, Model=LightGBM Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55658, Test samples: 13914\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 12\n",
      "points_prior: 96\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag108', 'rolling_mean_MUFL', 'MUFL_lag101', 'MUFL_lag104', 'MUFL_lag105', 'MUFL_lag106', 'MUFL_lag107', 'MUFL_lag108', 'rolling_mean_MULL', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag97', 'LULL_lag98', 'LULL_lag100', 'LULL_lag108', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 600, 'model__num_leaves': 63, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 13.070303603170762\n",
      "Final Test RMSE: 6.7100\n",
      "Train/Val samples: 55648, Test samples: 13912\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 24\n",
      "points_prior: 96\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'rolling_mean_MUFL', 'rolling_std_MUFL', 'MUFL_lag99', 'MUFL_lag104', 'MUFL_lag108', 'MUFL_lag109', 'MUFL_lag110', 'MUFL_lag115', 'MUFL_lag120', 'rolling_mean_MULL', 'rolling_mean_LUFL', 'LULL_lag97', 'LULL_lag98', 'LULL_lag120', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 600, 'model__num_leaves': 63, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 13.022509562720542\n",
      "Final Test RMSE: 6.6794\n",
      "Train/Val samples: 55629, Test samples: 13907\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 48\n",
      "points_prior: 96\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_std_HUFL', 'rolling_std_MUFL', 'MUFL_lag106', 'MUFL_lag107', 'MUFL_lag109', 'MUFL_lag110', 'MUFL_lag112', 'MUFL_lag136', 'MUFL_lag137', 'MUFL_lag140', 'MUFL_lag142', 'rolling_mean_MULL', 'LULL_lag139', 'LULL_lag142', 'LULL_lag143', 'LULL_lag144', 'year', 'month', 'day', 'dow']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.03, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 13.892274255316456\n",
      "Final Test RMSE: 6.5638\n",
      "Model saved to Model_LightGBM Regressor N_48 points_prior_96 dataset_trans_1.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_1.csv, points_prior: 96, Model LightGBM Regressor ***\n",
      "Best N: \t\t48\n",
      "Best Test RMSE: \t6.5638\n",
      "Best Hyperparameters: \t{'model__learning_rate': 0.03, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_1.csv, PointsPrior=672, Model=LightGBM Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55197, Test samples: 13799\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 12\n",
      "points_prior: 672\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HULL', 'rolling_mean_MUFL', 'MUFL_lag681', 'MUFL_lag682', 'MUFL_lag683', 'MUFL_lag684', 'rolling_mean_MULL', 'MULL_lag673', 'rolling_mean_LUFL', 'LUFL_lag684', 'rolling_mean_LULL', 'LULL_lag673', 'LULL_lag675', 'LULL_lag683', 'LULL_lag684', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.03, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 14.023145776078163\n",
      "Final Test RMSE: 6.0078\n",
      "Train/Val samples: 55188, Test samples: 13796\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 24\n",
      "points_prior: 672\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HULL', 'rolling_mean_MUFL', 'MUFL_lag678', 'MUFL_lag679', 'MUFL_lag680', 'MUFL_lag681', 'MUFL_lag683', 'MUFL_lag685', 'MUFL_lag686', 'MUFL_lag688', 'rolling_mean_MULL', 'MULL_lag673', 'rolling_mean_LULL', 'LULL_lag673', 'LULL_lag696', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.03, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 14.148105624114493\n",
      "Final Test RMSE: 6.0005\n",
      "Train/Val samples: 55168, Test samples: 13792\n",
      "Training Configurations\n",
      "file: trans_1.csv\n",
      "N: 48\n",
      "points_prior: 672\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HULL', 'MUFL_lag679', 'MUFL_lag683', 'MUFL_lag684', 'MUFL_lag685', 'MUFL_lag687', 'MUFL_lag688', 'rolling_mean_MULL', 'MULL_lag673', 'MULL_lag674', 'rolling_mean_LUFL', 'LULL_lag673', 'LULL_lag715', 'LULL_lag716', 'LULL_lag719', 'LULL_lag720', 'year', 'month', 'day', 'dow']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 600, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 14.273931574880596\n",
      "Final Test RMSE: 6.2508\n",
      "Model saved to Model_LightGBM Regressor N_24 points_prior_672 dataset_trans_1.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_1.csv, points_prior: 672, Model LightGBM Regressor ***\n",
      "Best N: \t\t24\n",
      "Best Test RMSE: \t6.0005\n",
      "Best Hyperparameters: \t{'model__learning_rate': 0.03, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_2.csv, PointsPrior=4, Model=LightGBM Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55732, Test samples: 13932\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 12\n",
      "points_prior: 4\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag5', 'HUFL_lag6', 'HUFL_lag8', 'HUFL_lag9', 'rolling_mean_MUFL', 'MUFL_lag5', 'MUFL_lag6', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag14', 'MULL_lag16', 'rolling_mean_LUFL', 'LUFL_lag5', 'LUFL_lag6', 'rolling_mean_LULL', 'year', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 600, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 14.846998803100064\n",
      "Final Test RMSE: 5.6434\n",
      "Train/Val samples: 55722, Test samples: 13930\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 24\n",
      "points_prior: 4\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag12', 'rolling_std_MUFL', 'MUFL_lag5', 'MUFL_lag6', 'MUFL_lag7', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag7', 'MULL_lag8', 'MULL_lag9', 'MULL_lag10', 'MULL_lag11', 'MULL_lag13', 'LUFL_lag5', 'rolling_mean_LULL', 'year', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 600, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 15.038513454103487\n",
      "Final Test RMSE: 5.8604\n",
      "Train/Val samples: 55703, Test samples: 13925\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 48\n",
      "points_prior: 4\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_std_MUFL', 'MUFL_lag5', 'MUFL_lag6', 'MUFL_lag7', 'MUFL_lag42', 'MUFL_lag44', 'MUFL_lag45', 'MUFL_lag47', 'rolling_mean_MULL', 'MULL_lag5', 'MULL_lag6', 'MULL_lag7', 'MULL_lag8', 'MULL_lag9', 'MULL_lag10', 'MULL_lag11', 'year', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.03, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 14.420809000846349\n",
      "Final Test RMSE: 5.5461\n",
      "Model saved to Model_LightGBM Regressor N_48 points_prior_4 dataset_trans_2.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_2.csv, points_prior: 4, Model LightGBM Regressor ***\n",
      "Best N: \t\t48\n",
      "Best Test RMSE: \t5.5461\n",
      "Best Hyperparameters: \t{'model__learning_rate': 0.03, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_2.csv, PointsPrior=96, Model=LightGBM Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55658, Test samples: 13914\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 12\n",
      "points_prior: 96\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag97', 'rolling_mean_HULL', 'HULL_lag97', 'rolling_mean_MUFL', 'MUFL_lag97', 'MUFL_lag98', 'MUFL_lag99', 'rolling_mean_MULL', 'MULL_lag99', 'MULL_lag107', 'MULL_lag108', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag97', 'LULL_lag107', 'LULL_lag108', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.03, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 13.395465992281677\n",
      "Final Test RMSE: 5.7138\n",
      "Train/Val samples: 55648, Test samples: 13912\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 24\n",
      "points_prior: 96\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_MUFL', 'MUFL_lag97', 'MUFL_lag98', 'MUFL_lag99', 'MUFL_lag100', 'rolling_mean_MULL', 'MULL_lag97', 'MULL_lag98', 'MULL_lag100', 'MULL_lag120', 'rolling_mean_LUFL', 'rolling_mean_LULL', 'LULL_lag110', 'LULL_lag112', 'LULL_lag120', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 14.560502384993535\n",
      "Final Test RMSE: 5.7809\n",
      "Train/Val samples: 55629, Test samples: 13907\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 48\n",
      "points_prior: 96\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_std_MUFL', 'MUFL_lag97', 'MUFL_lag98', 'MUFL_lag139', 'MUFL_lag140', 'MUFL_lag141', 'MUFL_lag142', 'MUFL_lag144', 'rolling_mean_MULL', 'MULL_lag97', 'MULL_lag98', 'MULL_lag100', 'MULL_lag101', 'MULL_lag103', 'MULL_lag105', 'MULL_lag106', 'MULL_lag107', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.03, 'model__n_estimators': 300, 'model__num_leaves': 63, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 13.495210300176366\n",
      "Final Test RMSE: 6.5689\n",
      "Model saved to Model_LightGBM Regressor N_12 points_prior_96 dataset_trans_2.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_2.csv, points_prior: 96, Model LightGBM Regressor ***\n",
      "Best N: \t\t12\n",
      "Best Test RMSE: \t5.7138\n",
      "Best Hyperparameters: \t{'model__learning_rate': 0.03, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      "Processing: File=trans_2.csv, PointsPrior=672, Model=LightGBM Regressor\n",
      "========================================================\n",
      "\n",
      "Train/Val samples: 55197, Test samples: 13799\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 12\n",
      "points_prior: 672\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['rolling_mean_HUFL', 'HUFL_lag673', 'rolling_mean_HULL', 'HULL_lag673', 'HULL_lag683', 'HULL_lag684', 'MUFL_lag673', 'rolling_mean_MULL', 'MULL_lag673', 'rolling_mean_LULL', 'LULL_lag673', 'LULL_lag674', 'LULL_lag679', 'LULL_lag683', 'LULL_lag684', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.03, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 15.103789142738547\n",
      "Final Test RMSE: 6.3516\n",
      "Train/Val samples: 55188, Test samples: 13796\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 24\n",
      "points_prior: 672\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag673', 'HUFL_lag674', 'rolling_mean_HULL', 'HULL_lag673', 'HULL_lag674', 'HULL_lag692', 'HULL_lag695', 'HULL_lag696', 'rolling_mean_MULL', 'MULL_lag673', 'LUFL_lag687', 'LULL_lag673', 'LULL_lag688', 'LULL_lag694', 'LULL_lag695', 'year', 'month', 'day', 'dow', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.03, 'model__n_estimators': 600, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 13.873230196822048\n",
      "Final Test RMSE: 6.3634\n",
      "Train/Val samples: 55168, Test samples: 13792\n",
      "Training Configurations\n",
      "file: trans_2.csv\n",
      "N: 48\n",
      "points_prior: 672\n",
      "model: LightGBM Regressor\n",
      "Selecting top 20 features...\n",
      "Top 20 selected features:\n",
      " ['HUFL_lag673', 'rolling_mean_HULL', 'HULL_lag673', 'HULL_lag674', 'HULL_lag675', 'HULL_lag676', 'HULL_lag677', 'HULL_lag679', 'HULL_lag680', 'HULL_lag682', 'HULL_lag683', 'MUFL_lag709', 'MUFL_lag710', 'MUFL_lag716', 'rolling_mean_MULL', 'MULL_lag673', 'MULL_lag674', 'month', 'day', 'hour']\n",
      "Training data using train and val...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "Best CV RMSE: 13.566164263977619\n",
      "Final Test RMSE: 6.2871\n",
      "Model saved to Model_LightGBM Regressor N_48 points_prior_672 dataset_trans_2.csv.pkl\n",
      "\n",
      "--------------------------------------------------------\n",
      "*** BEST N for trans_2.csv, points_prior: 672, Model LightGBM Regressor ***\n",
      "Best N: \t\t48\n",
      "Best Test RMSE: \t6.2871\n",
      "Best Hyperparameters: \t{'model__learning_rate': 0.1, 'model__n_estimators': 300, 'model__num_leaves': 31, 'model__reg_lambda': 1.0}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "           FINAL SUMMARY           \n",
      "===================================\n",
      "Config (File, Prior, Model): ('trans_1.csv', 4, 'LightGBM Regressor')\n",
      "  -> Best N: 12 (RMSE: 6.3048)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_1.csv', 96, 'LightGBM Regressor')\n",
      "  -> Best N: 48 (RMSE: 6.5638)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_1.csv', 672, 'LightGBM Regressor')\n",
      "  -> Best N: 24 (RMSE: 6.0005)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_2.csv', 4, 'LightGBM Regressor')\n",
      "  -> Best N: 48 (RMSE: 5.5461)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_2.csv', 96, 'LightGBM Regressor')\n",
      "  -> Best N: 12 (RMSE: 5.7138)\n",
      "\n",
      "Config (File, Prior, Model): ('trans_2.csv', 672, 'LightGBM Regressor')\n",
      "  -> Best N: 48 (RMSE: 6.2871)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "models = [\n",
    "        [\n",
    "            \"LightGBM Regressor\",\n",
    "            Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('model', LGBMRegressor(\n",
    "                    random_state=42,\n",
    "                    objective='regression',\n",
    "                    verbose=-1\n",
    "                ))\n",
    "            ]),\n",
    "            {\n",
    "                \"model__num_leaves\": [31, 63],\n",
    "                \"model__learning_rate\": [0.03, 0.1],\n",
    "                \"model__n_estimators\": [300, 600],\n",
    "                \"model__reg_lambda\": [1.0],\n",
    "            }\n",
    "        ]\n",
    "]\n",
    "\n",
    "N_list = [12, 24, 48] # can add 96 if you want (and have enough time)\n",
    "points_priors = [4, 96, 672]\n",
    "filenames = [\"trans_1.csv\", \"trans_2.csv\"]\n",
    "\n",
    "# Store the overall best N for each configuration\n",
    "overall_best_results = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    for points_prior in points_priors:\n",
    "        for (model_name, model, model_params) in models:\n",
    "\n",
    "            print(f\"\\n========================================================\")\n",
    "            print(f\"Processing: File={filename}, PointsPrior={points_prior}, Model={model_name}\")\n",
    "            print(f\"========================================================\\n\")\n",
    "\n",
    "            results_per_n = {}\n",
    "            for N in N_list:\n",
    "                df = preprocess_data(filename, N = N, points_prior= points_prior)\n",
    "                X_trainval, y_trainval, X_test, y_test, tscv = time_series_split(df, \"OT\")\n",
    "\n",
    "                print(\"Training Configurations\")\n",
    "                print(f\"file: {filename}\")\n",
    "                print(f\"N: {N}\")\n",
    "                print(f\"points_prior: {points_prior}\")\n",
    "                print(f\"model: {model_name}\")\n",
    "\n",
    "                # ---- FEATURE SELECTION using RFE ----\n",
    "                base_model = XGBRegressor(\n",
    "                    objective='reg:squarederror',\n",
    "                    random_state=42,\n",
    "                    n_estimators=200,\n",
    "                    learning_rate=0.05,\n",
    "                    max_depth=5,\n",
    "                    subsample=0.8\n",
    "                )\n",
    "\n",
    "                # Select top 20 features\n",
    "                print(\"Selecting top 20 features...\")\n",
    "                rfe = RFE(estimator=base_model, n_features_to_select=20, step=0.1)\n",
    "                rfe.fit(X_trainval, y_trainval)\n",
    "\n",
    "                # Mask of selected features\n",
    "                selected_features = list(X_trainval.columns[rfe.support_])\n",
    "                print(\"Top 20 selected features:\\n\", list(selected_features))\n",
    "\n",
    "                # Filter columns\n",
    "                X_trainval_sel = X_trainval[selected_features]\n",
    "                X_test_sel = X_test[selected_features]\n",
    "\n",
    "                # GridSearchCV with TimeSeriesSplit\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_grid=model_params,\n",
    "                    cv=tscv,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "\n",
    "                # Fit using only train/val (no test!)\n",
    "                print(\"Training data using train and val...\")\n",
    "                grid_search.fit(X_trainval_sel, y_trainval)\n",
    "\n",
    "                # Show CV results\n",
    "                print(\"Best parameters:\", grid_search.best_params_)\n",
    "                print(\"Best CV RMSE:\", np.sqrt(-grid_search.best_score_))\n",
    "\n",
    "                # ---- Final evaluation on the hold-out test set ----\n",
    "                best_model = grid_search.best_estimator_\n",
    "                y_pred = best_model.predict(X_test_sel)\n",
    "                test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "                print(f\"Final Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "                results_per_n[N] = {\n",
    "                    \"rmse\": test_rmse,\n",
    "                    \"best_params\": grid_search.best_params_,\n",
    "                    \"selected_features\": selected_features,\n",
    "                    \"best_model\" : best_model\n",
    "                }\n",
    "\n",
    "            best_n_value = min(results_per_n.keys(), key=lambda n: results_per_n[n][\"rmse\"])\n",
    "            best_result = results_per_n[best_n_value]\n",
    "            best_model = best_result[\"best_model\"]\n",
    "\n",
    "            # saving best model\n",
    "            model_filename = f\"Model_{model_name} N_{best_n_value} points_prior_{points_prior} dataset_{filename}.pkl\"\n",
    "            with open(model_filename, 'wb') as file:\n",
    "                pickle.dump(best_model, file)\n",
    "\n",
    "            print(f\"Model saved to {model_filename}\")\n",
    "            \n",
    "            print(f\"\\n--------------------------------------------------------\")\n",
    "            print(f\"*** BEST N for {filename}, points_prior: {points_prior}, Model {model_name} ***\")\n",
    "            print(f\"Best N: \\t\\t{best_n_value}\")\n",
    "            print(f\"Best Test RMSE: \\t{best_result['rmse']:.4f}\")\n",
    "            print(f\"Best Hyperparameters: \\t{best_result['best_params']}\")\n",
    "            print(f\"--------------------------------------------------------\\n\")\n",
    "            \n",
    "            # Store this in the overall results for a final summary\n",
    "            config_key = (filename, points_prior, model_name)\n",
    "            overall_best_results[config_key] = {\n",
    "                \"best_n\": best_n_value,\n",
    "                \"rmse\": best_result['rmse'],\n",
    "                \"best_params\": best_result['best_params'],\n",
    "                \"selected_features\": best_result['selected_features']\n",
    "            }\n",
    "\n",
    "# 4. Print a final summary of all configurations\n",
    "print(\"\\n\\n===================================\")\n",
    "print(\"           FINAL SUMMARY           \")\n",
    "print(\"===================================\")\n",
    "for config, result in overall_best_results.items():\n",
    "    print(f\"Config (File, Prior, Model): {config}\")\n",
    "    print(f\"  -> Best N: {result['best_n']} (RMSE: {result['rmse']:.4f})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
